{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e541dd",
   "metadata": {},
   "source": [
    "## Step 1: BetterPrompt for perplexity scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\" # fill in your own key \n",
    "import betterprompt\n",
    "prompt = \"This is a sample prompt\"\n",
    "perplexity = betterprompt.calculate_perplexity(prompt)\n",
    "\n",
    "# Get perplexity scores for prompt paraphrases\n",
    "prompt_paraphrase_file = \"../../UW-Health-Prompt/prompt_paraphrase_v3.txt\"\n",
    "subject_matter_prompt1 = open(prompt_paraphrase_file).readlines() \n",
    "prompt_score = {}\n",
    "for k,v in enumerate(subject_matter_prompt1):\n",
    "    prompt = v\n",
    "    perplexity = betterprompt.calculate_perplexity(prompt)\n",
    "    prompt_score[k] = perplexity\n",
    "    \n",
    "for k,v in prompt_score.items():\n",
    "    #print(k,v)\n",
    "    print(f\"{v:.2e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(prompt_score.items(), key=lambda x: x[1])\n",
    "\n",
    "# Extract the keys of the top 5 smallest values\n",
    "smallest_keys = [key for key, value in sorted_dict[:5]]\n",
    "\n",
    "# Print the result\n",
    "print(smallest_keys)\n",
    "\n",
    "import numpy as np \n",
    "avg_sc = np.mean(list(prompt_score.values()))\n",
    "print(f\"Avg Perplexity: {avg_sc:.2e}\")\n",
    "\n",
    "lowest_prompts = [] \n",
    "for _,i in enumerate(smallest_keys):\n",
    "    print(f\"Prompt {i} with Perplexity: {prompt_score[i]:.2e} \\n > : {subject_matter_prompt1[i]}  === \")\n",
    "    lowest_prompts.append(subject_matter_prompt1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228ac50",
   "metadata": {},
   "source": [
    "## Step 2: Define self-consistency metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4aa222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ygao/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True) \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def measure_self_consistency_auto_jaccard(response1, response2,mode=\"jaccard\"):\n",
    "    prompt_tokens = set(word_tokenize(response1.lower()))\n",
    "    response_tokens = set(word_tokenize(response2.lower()))\n",
    "    \n",
    "    jaccard_score = jaccard_similarity(prompt_tokens, response_tokens)\n",
    "    \n",
    "    return jaccard_score \n",
    "\n",
    "def measure_self_consistency_auto_bert(string1, string2, model, tokenizer):\n",
    "    # Tokenize and encode the strings\n",
    "    input_encoding1 = model(**tokenizer(string1,return_tensors='pt', max_length=256, padding=True, truncation=True))\n",
    "    input_encoding2 = model(**tokenizer(string2,return_tensors='pt', max_length=256, padding=True, truncation=True))\n",
    "    # Get the model's output\n",
    "    with torch.no_grad():\n",
    "        embeddings1= input_encoding1.pooler_output.detach().numpy()\n",
    "        embeddings2= input_encoding2.pooler_output.detach().numpy()\n",
    "     \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(embeddings1, embeddings2) \n",
    "    \n",
    "    return cosine_sim[0][0]  # Return the cosine similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for chatgpt API call \n",
    "\n",
    "def call_chatgpt_system(chatgpt_input_content,system_prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      engine=\"test_chatgpt\",\n",
    "      messages = [{\"role\":\"system\", \"content\":system_prompt},#\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                  {\"role\":\"user\",\"content\":chatgpt_input_content}],\n",
    "                  # {\"role\":\"assistant\",\"content\":\"Microsoft was founded by Bill Gates and Paul Allen in 1975.\"}],\n",
    "      temperature=0.5,\n",
    "      max_tokens=160,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "\n",
    "    #print(response)\n",
    "    #print(response['choices'][0]['message']['content'])\n",
    "    #print(\"Target Gold: \", target_gold)\n",
    "    output = response['choices'][0]['message']['content']\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_categories = {}\n",
    "\n",
    "# pipeline to read fewshot example for self-consistency; note that below is the UW Mychart message usecase  \n",
    "\n",
    "category = \"results\" # medication, general, paperwork, results \n",
    "if_fewshot = \"yes\"\n",
    "\n",
    "def extract_input_example(category):\n",
    "    example_file = open(\"../../UW-Health-Prompt/\"+category+\"_example.txt\").readlines()\n",
    "    example_dict = {}\n",
    "    for l in example_file:\n",
    "        cleaned_string = l.replace(\"||\", \"\")\n",
    "        #print(cleaned_string)\n",
    "        doc_pat = cleaned_string.split(\"<Doctor>\")\n",
    "        #print(doc_pat)\n",
    "        if len(doc_pat) == 2:\n",
    "            key = doc_pat[0].split(\"<Patient>\", 1)[-1] \n",
    "            example_dict[key] = doc_pat[-1]\n",
    "    print(f\"Extract {len(example_dict)} examples for self-consistency measures\")\n",
    "    return example_dict \n",
    "\n",
    "categories = []  \n",
    "categories = ['general', 'medication', 'results', 'paperwork'] \n",
    "for category in categories:\n",
    "    fewshot_file = open(\"../../UW-Health-Prompt/\"+category+\"_fewshot.txt\").read()\n",
    "    example_dict = extract_input_example(category)\n",
    "\n",
    "    jaccard_avg = [] \n",
    "    prompt_scores = {}\n",
    "    prompt_output = {} \n",
    "\n",
    "    consistency_sample = 5 \n",
    "\n",
    "    for prompt in lowest_prompts:\n",
    "        prompt_scores[prompt] = []\n",
    "        prompt_output[prompt] = [] \n",
    "        for eg_input, eg_output in example_dict.items():\n",
    "            if if_fewshot:\n",
    "                chatgpt_input = fewshot_file + eg_input \n",
    "            else:\n",
    "                chatgpt_input = eg_input \n",
    "            tmp = [] \n",
    "            tmp_output = [] \n",
    "            for _ in range(consistency_sample):\n",
    "                output = call_chatgpt_system(chatgpt_input, prompt)\n",
    "                #score = measure_self_consistency_auto(eg_output, prompt) \n",
    "                score = measure_self_consistency_auto_bert(eg_output, output, model, tokenizer)\n",
    "                tmp.append(score)\n",
    "                tmp_output.append(output)\n",
    "            prompt_scores[prompt].append(tmp)\n",
    "            prompt_output[prompt].append(tmp_output)\n",
    "\n",
    "    print(category)\n",
    "    prompt_categories[category] = [prompt_scores, prompt_output]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b170f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results from the four categories and run bootstrap \n",
    "aggr_results = {}\n",
    "\n",
    "for k,v in best_sc_prompt.items():\n",
    "    aggr_results[k] = []\n",
    "    tmp = [] \n",
    "    for kk,vv in v.items():\n",
    "        tmp.extend(vv)\n",
    "    aggr_results[k] = tmp\n",
    "\n",
    "import random\n",
    "for k,v in aggr_results.items():\n",
    "    print(f\"current prompt: {k}\")\n",
    "    print(f\"AVG: {np.mean(v)*100:.2f} STD: {np.std(v)*100:.2f}\")\n",
    "    \n",
    "bootstrapped_n = 100\n",
    "num_samples = len(v) \n",
    "bootstrapp_index = [] \n",
    "for _ in range(0, num_samples):\n",
    "    k = random.randint(0, num_samples - 1)\n",
    "    bootstrapp_index.append(k)\n",
    "\n",
    "aggr_bootstrap_results = {}\n",
    "for k,v in aggr_results.items():\n",
    "    aggr_bootstrap_results[k] = []\n",
    "    for _ in bootstrapp_index:\n",
    "        aggr_bootstrap_results[k].append(v[_])\n",
    "        \n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h \n",
    "\n",
    "aggr_ci_results = {}\n",
    "for k,v in aggr_bootstrap_results.items():\n",
    "    print(f\"Prompt: {k}\")\n",
    "    mean, upper, lower = mean_confidence_interval(v)\n",
    "    aggr_ci_results[k] = [mean, upper, lower]\n",
    "    print(f\"Mean: {mean:.4f} Upper: {upper:.4f} Lower: {lower:.4f}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
